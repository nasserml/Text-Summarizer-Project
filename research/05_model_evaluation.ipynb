{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code is a Python notebook that performs model evaluation for a text summarization project. Here's a summary of the code:\n",
    "\n",
    "1. The code imports the necessary libraries and sets the working directory to the project root directory.\n",
    "\n",
    "2. It defines a dataclass `ModelEvaluationConfig` that represents the configuration for model evaluation. It includes paths for data, model, tokenizer, and the metric file.\n",
    "\n",
    "3. The `ConfigurationManager` class handles the reading of YAML configuration files and creation of necessary directories.\n",
    "\n",
    "4. The `get_model_evaluation_config` method in `ConfigurationManager` retrieves the model evaluation configuration and creates the `ModelEvaluationConfig` object.\n",
    "\n",
    "5. The code imports libraries for the evaluation process, including `transformers`, `datasets`, `torch`, `pandas`, and `tqdm`.\n",
    "\n",
    "6. The `ModelEvaluation` class is defined, which takes the `ModelEvaluationConfig` as input.\n",
    "\n",
    "7. The `ModelEvaluation` class has a method called `generate_batch_sized_chunks` to split the dataset into smaller batches for processing.\n",
    "\n",
    "8. The `calculate_metric_on_test_ds` method in `ModelEvaluation` calculates the evaluation metric (ROUGE score) on the test dataset using the provided model and tokenizer.\n",
    "\n",
    "9. The `evaluate` method in `ModelEvaluation` initializes the tokenizer and model, loads the test dataset, and calculates the ROUGE score using the `calculate_metric_on_test_ds` method.\n",
    "\n",
    "10. The calculated ROUGE scores are stored in a DataFrame and saved to the metric file specified in the configuration.\n",
    "\n",
    "11. The code instantiates the `ConfigurationManager`, retrieves the model evaluation configuration, creates an instance of `ModelEvaluation`, and performs model evaluation by calling the `evaluate` method.\n",
    "\n",
    "Overall, this code performs model evaluation using the ROUGE metric on a text summarization model. It retrieves the necessary configurations, initializes the model and tokenizer, loads the test dataset, calculates the metric, and saves the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\artificial intelegnce\\\\study\\\\ML End To End Projects Krish Naik\\\\github\\\\Text-Summarizer-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\artificial intelegnce\\\\study\\\\ML End To End Projects Krish Naik\\\\github\\\\Text-Summarizer-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_path = config.model_path,\n",
    "            tokenizer_path = config.tokenizer_path,\n",
    "            metric_file_name = config.metric_file_name\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "    \n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "            \n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "            \n",
    "            # Finally, we decode the generated texts, \n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=True) \n",
    "                for s in summaries]      \n",
    "            \n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            \n",
    "            \n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "            \n",
    "        #  Finally compute and return the ROUGE scores.\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "       \n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "  \n",
    "        rouge_metric = load_metric('rouge')\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    "            )\n",
    "\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n",
    "        df.to_csv(self.config.metric_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-19 23:35:52,046: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-06-19 23:35:52,057: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-06-19 23:35:52,058: INFO: common: created directory at: artifacts]\n",
      "[2023-06-19 23:35:52,060: INFO: common: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nasser\\AppData\\Local\\Temp\\ipykernel_36052\\2035518382.py:59: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n",
      "Downloading builder script: 5.65kB [00:00, 39.2MB/s]                   \n",
      "100%|██████████| 5/5 [03:12<00:00, 38.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-19 23:39:35,980: INFO: rouge_scorer: Using default tokenizer.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "from Text-Summarization-NLP-Project.research.05_Model_evaluation.ipynb import os\n",
    "```\n",
    "This line imports the `os` module, which provides a way to use operating system-dependent functionality.\n",
    "\n",
    "```python\n",
    "%pwd\n",
    "```\n",
    "This line is a Jupyter Notebook magic command that displays the current working directory.\n",
    "\n",
    "```python\n",
    "'d:\\\\Bappy\\\\YouTube\\\\Text-Summarizer-Project\\\\research'\n",
    "```\n",
    "This is the output of the `%pwd` command, indicating the current working directory.\n",
    "\n",
    "```python\n",
    "os.chdir(\"../\")\n",
    "```\n",
    "This line changes the current working directory to the parent directory.\n",
    "\n",
    "```python\n",
    "%pwd\n",
    "```\n",
    "This line again displays the current working directory to confirm the change.\n",
    "\n",
    "```python\n",
    "'d:\\\\Bappy\\\\YouTube\\\\Text-Summarizer-Project'\n",
    "```\n",
    "This is the updated output of the `%pwd` command, indicating the new current working directory.\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "```\n",
    "These lines import the `dataclass` decorator from the `dataclasses` module and the `Path` class from the `pathlib` module.\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path\n",
    "```\n",
    "This code defines a data class `ModelEvaluationConfig` using the `@dataclass` decorator. It represents the configuration for model evaluation and has five attributes: `root_dir`, `data_path`, `model_path`, `tokenizer_path`, and `metric_file_name`, all of which are of type `Path`.\n",
    "\n",
    "```python\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories\n",
    "```\n",
    "These lines import constants and utility functions from other modules (`constants.py` and `common.py`) of a package called `textSummarizer`.\n",
    "\n",
    "```python\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "```\n",
    "This code defines a class `ConfigurationManager` that handles the configuration and parameters for the model evaluation. It initializes the configuration and parameters by reading YAML files (`config_filepath` and `params_filepath`) and creates necessary directories based on the configuration.\n",
    "\n",
    "```python\n",
    "def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "    config = self.config.model_evaluation\n",
    "    create_directories([config.root_dir])\n",
    "    model_evaluation_config = ModelEvaluationConfig(\n",
    "        root_dir=config.root_dir,\n",
    "        data_path=config.data_path,\n",
    "        model_path=config.model_path,\n",
    "        tokenizer_path=config.tokenizer_path,\n",
    "        metric_file_name=config.metric_file_name\n",
    "    )\n",
    "    return model_evaluation_config\n",
    "```\n",
    "This method in the `ConfigurationManager` class retrieves the model evaluation configuration from the overall configuration and creates an instance of `ModelEvaluationConfig` based on the retrieved values.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "```\n",
    "These lines import necessary libraries and modules for the model evaluation process, including the Transformers library, the Datasets library, Torch, Pandas, and tqdm.\n",
    "\n",
    "```python\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "```\n",
    "This code defines a class `ModelEvaluation` that represents the model\n",
    "\n",
    " evaluation process. It takes an instance of `ModelEvaluationConfig` as input and stores it as a configuration attribute.\n",
    "\n",
    "```python\n",
    "def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i: i + batch_size]\n",
    "```\n",
    "This method in the `ModelEvaluation` class splits a given list of elements into smaller batches of a specified size. It uses a generator to yield successive batch-sized chunks.\n",
    "\n",
    "```python\n",
    "def calculate_metric_on_test_ds(self, dataset, metric, model, tokenizer, batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "    # ... Rest of the code ...\n",
    "```\n",
    "This method in the `ModelEvaluation` class calculates the evaluation metric (ROUGE score) on the test dataset. It takes the dataset, metric, model, tokenizer, and other parameters as input. It splits the dataset into smaller batches using the `generate_batch_sized_chunks` method.\n",
    "\n",
    "```python\n",
    "def evaluate(self):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "    model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "    dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    rouge_metric = load_metric('rouge')\n",
    "    score = self.calculate_metric_on_test_ds(dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size=2, column_text='dialogue', column_summary='summary')\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    df = pd.DataFrame(rouge_dict, index=['pegasus'])\n",
    "    df.to_csv(self.config.metric_file_name, index=False)\n",
    "```\n",
    "This method in the `ModelEvaluation` class performs the model evaluation process. It initializes the tokenizer and the Pegasus model, loads the test dataset, calculates the ROUGE score using the `calculate_metric_on_test_ds` method, and saves the results to a CSV file specified in the configuration.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.evaluate()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "```\n",
    "This code block creates an instance of the `ConfigurationManager`, retrieves the model evaluation configuration, creates an instance of `ModelEvaluation` with the retrieved configuration, and performs the model evaluation by calling the `evaluate` method. If any exception occurs during the process, it is raised."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22fe6abd7fa6c55aa584987fd47a2d98eb23e8b764dd6e6419fc0b943942d875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
