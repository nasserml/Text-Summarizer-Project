{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `import os`: Imports the `os` module for operating system-related functionalities.\n",
    "- `%pwd`: Displays the current working directory.\n",
    "- `os.chdir(\"../\")`: Changes the current working directory to the parent directory.\n",
    "- `%pwd`: Displays the current working directory again to verify the change.\n",
    "- `from dataclasses import dataclass, Path`: Imports the `dataclass` decorator and the `Path` class from the `dataclasses` and `pathlib` modules, respectively.\n",
    "- `@dataclass(frozen=True)`: Decorator that creates an immutable data class `DataTransformationConfig` with the specified attributes.\n",
    "- `class ConfigurationManager`: Defines the `ConfigurationManager` class responsible for managing the project's configuration.\n",
    "- `def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH)`: Initializes the `ConfigurationManager` instance with optional `config_filepath` and `params_filepath` parameters, which default to predefined constants.\n",
    "- `self.config = read_yaml(config_filepath)`: Reads the configuration file specified by `config_filepath` using the `read_yaml` function and assigns the result to the `config` attribute.\n",
    "- `self.params = read_yaml(params_filepath)`: Reads the parameters file specified by `params_filepath` using the `read_yaml` function and assigns the result to the `params` attribute.\n",
    "- `create_directories([self.config.artifacts_root])`: Creates directories specified in the configuration file using the `create_directories` function.\n",
    "- `def get_data_transformation_config(self) -> DataTransformationConfig`: Retrieves the data transformation configuration by calling the `get_data_transformation_config` method of the `ConfigurationManager` instance.\n",
    "- `create_directories([config.root_dir])`: Creates directories specified in the data transformation configuration.\n",
    "- `data_transformation_config = DataTransformationConfig(...)`: Creates an instance of the `DataTransformationConfig` class using the data transformation configuration values.\n",
    "- `import os`: Imports the `os` module.\n",
    "- `from textSummarizer.logging import logger`: Imports the `logger` object from the `textSummarizer.logging` module, which is used for logging.\n",
    "- `from transformers import AutoTokenizer`: Imports the `AutoTokenizer` class from the `transformers` module, which is used for tokenization.\n",
    "- `from datasets import load_dataset, load_from_disk`: Imports the `load_dataset` and `load_from_disk` functions from the `datasets` module, which are used for loading datasets.\n",
    "- `class DataTransformation`: Defines the `DataTransformation` class responsible for converting data into features for model training.\n",
    "- `def __init__(self, config: DataTransformationConfig)`: Initializes the `DataTransformation` instance with a `config` parameter of type `DataTransformationConfig`.\n",
    "- `self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)`: Creates an instance of the `AutoTokenizer` class, initialized with the tokenizer specified in the configuration, and assigns it to the `tokenizer` attribute.\n",
    "- `def convert_examples_to_features(self, example_batch)`: Converts examples from the dataset to features for model training.\n",
    "- `input_encodings = self.tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)`: Tokenizes the dialogue from the example batch using the `tokenizer` attribute.\n",
    "- `with self.tokenizer.as_target_tokenizer():`: Context manager for using the tokenizer as the target tokenizer.\n",
    "- `target_encodings = self.tokenizer(example_batch['summary'], max_length=128, truncation=True)`: Tokenizes the summary from the example batch using the target tokenizer.\n",
    "- `def convert(self)`: Converts the dataset to features and saves it to disk.\n",
    "- `dataset_samsum = load_from_disk(self.config.data_path)`: Loads the dataset from disk using the data path specified in the configuration.\n",
    "- `dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched=True)`: Maps the `convert_examples_to_features` method to the dataset to convert examples to features in batches.\n",
    "- `dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir, \"samsum_dataset\"))`: Saves the converted dataset to disk at the specified path.\n",
    "- `config = ConfigurationManager()`: Creates an instance of the `ConfigurationManager` class to manage the project's configuration.\n",
    "- `data_transformation_config = config.get_data_transformation_config()`: Retrieves the data transformation configuration by calling the `get_data_transformation_config` method of the `ConfigurationManager` instance.\n",
    "- `data_transformation = DataTransformation(config=data_transformation_config)`: Creates an instance of the `DataTransformation` class for data transformation, passing the obtained data transformation configuration as a parameter.\n",
    "- `data_transformation.convert()`: Invokes the `convert` method of the `DataTransformation` instance to perform the data transformation process.\n",
    "\n",
    "In summary, the code performs data transformation for the text summarization project. It sets up the necessary configurations, loads the dataset, tokenizes the input data, converts it into features, and saves the transformed dataset to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\artificial intelegnce\\\\study\\\\ML End To End Projects Krish Naik\\\\github\\\\Text-Summarizer-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\artificial intelegnce\\\\study\\\\ML End To End Projects Krish Naik\\\\github\\\\Text-Summarizer-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_name = config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textSummarizer.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_examples_to_features(self,example_batch):\n",
    "        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "            \n",
    "        return {\n",
    "            'input_ids' : input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "    \n",
    "\n",
    "    def convert(self):\n",
    "        dataset_samsum = load_from_disk(self.config.data_path)\n",
    "        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n",
    "        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,\"samsum_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-19 19:51:19,987: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-06-19 19:51:19,989: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-06-19 19:51:19,990: INFO: common: created directory at: artifacts]\n",
      "[2023-06-19 19:51:19,992: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 88.0/88.0 [00:00<00:00, 43.8kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 573kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 1.91M/1.91M [00:01<00:00, 1.29MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 32.6kB/s]\n",
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]c:\\ProgramData\\anaconda3\\envs\\textS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22fe6abd7fa6c55aa584987fd47a2d98eb23e8b764dd6e6419fc0b943942d875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
